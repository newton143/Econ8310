{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newton143/Econ8310/blob/master/Assignment3_Latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5lgmUUMQvVbA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU_EBJ64wCqI",
        "outputId": "5960e42f-b98a-4e51-87aa-3aade5baa393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.3MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 301kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.58MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 10.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load training and test data\n",
        "train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xTv04bePwyAw"
      },
      "outputs": [],
      "source": [
        "# FashionNet Neural Network\n",
        "class FashionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T8grjuj0w5df"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "model = FashionNet()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U0i0CUhQw6u1"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train(model, train_loader, loss_fn, optimizer, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        for batch, (X, y) in enumerate(train_loader):\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch}, Loss: {loss.item():.4f}\")\n",
        "    print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d9X80lL4xAWu"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "def test(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            pred = model(X).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmbavTszxEBD",
        "outputId": "cd318895-280b-48f0-a116-f65c50401f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 0, Loss: 0.1950\n",
            "Epoch 1, Batch 100, Loss: 0.4569\n",
            "Epoch 1, Batch 200, Loss: 0.2775\n",
            "Epoch 1, Batch 300, Loss: 0.1993\n",
            "Epoch 1, Batch 400, Loss: 0.3050\n",
            "Epoch 1, Batch 500, Loss: 0.2240\n",
            "Epoch 1, Batch 600, Loss: 0.2345\n",
            "Epoch 1, Batch 700, Loss: 0.1802\n",
            "Epoch 1, Batch 800, Loss: 0.1977\n",
            "Epoch 1, Batch 900, Loss: 0.3670\n",
            "Epoch 2, Batch 0, Loss: 0.3000\n",
            "Epoch 2, Batch 100, Loss: 0.3017\n",
            "Epoch 2, Batch 200, Loss: 0.1998\n",
            "Epoch 2, Batch 300, Loss: 0.1921\n",
            "Epoch 2, Batch 400, Loss: 0.2300\n",
            "Epoch 2, Batch 500, Loss: 0.2214\n",
            "Epoch 2, Batch 600, Loss: 0.1973\n",
            "Epoch 2, Batch 700, Loss: 0.3401\n",
            "Epoch 2, Batch 800, Loss: 0.1790\n",
            "Epoch 2, Batch 900, Loss: 0.2023\n",
            "Epoch 3, Batch 0, Loss: 0.2065\n",
            "Epoch 3, Batch 100, Loss: 0.4187\n",
            "Epoch 3, Batch 200, Loss: 0.2891\n",
            "Epoch 3, Batch 300, Loss: 0.3897\n",
            "Epoch 3, Batch 400, Loss: 0.3281\n",
            "Epoch 3, Batch 500, Loss: 0.1484\n",
            "Epoch 3, Batch 600, Loss: 0.1719\n",
            "Epoch 3, Batch 700, Loss: 0.1780\n",
            "Epoch 3, Batch 800, Loss: 0.2630\n",
            "Epoch 3, Batch 900, Loss: 0.2802\n",
            "Epoch 4, Batch 0, Loss: 0.1982\n",
            "Epoch 4, Batch 100, Loss: 0.2891\n",
            "Epoch 4, Batch 200, Loss: 0.1940\n",
            "Epoch 4, Batch 300, Loss: 0.1885\n",
            "Epoch 4, Batch 400, Loss: 0.1190\n",
            "Epoch 4, Batch 500, Loss: 0.2696\n",
            "Epoch 4, Batch 600, Loss: 0.3170\n",
            "Epoch 4, Batch 700, Loss: 0.2196\n",
            "Epoch 4, Batch 800, Loss: 0.2296\n",
            "Epoch 4, Batch 900, Loss: 0.2393\n",
            "Epoch 5, Batch 0, Loss: 0.2821\n",
            "Epoch 5, Batch 100, Loss: 0.3068\n",
            "Epoch 5, Batch 200, Loss: 0.2961\n",
            "Epoch 5, Batch 300, Loss: 0.1324\n",
            "Epoch 5, Batch 400, Loss: 0.2537\n",
            "Epoch 5, Batch 500, Loss: 0.1629\n",
            "Epoch 5, Batch 600, Loss: 0.2999\n",
            "Epoch 5, Batch 700, Loss: 0.3620\n",
            "Epoch 5, Batch 800, Loss: 0.2565\n",
            "Epoch 5, Batch 900, Loss: 0.1661\n",
            "Epoch 6, Batch 0, Loss: 0.2708\n",
            "Epoch 6, Batch 100, Loss: 0.1180\n",
            "Epoch 6, Batch 200, Loss: 0.1471\n",
            "Epoch 6, Batch 300, Loss: 0.2001\n",
            "Epoch 6, Batch 400, Loss: 0.3845\n",
            "Epoch 6, Batch 500, Loss: 0.2296\n",
            "Epoch 6, Batch 600, Loss: 0.1364\n",
            "Epoch 6, Batch 700, Loss: 0.2784\n",
            "Epoch 6, Batch 800, Loss: 0.2383\n",
            "Epoch 6, Batch 900, Loss: 0.1435\n",
            "Epoch 7, Batch 0, Loss: 0.3084\n",
            "Epoch 7, Batch 100, Loss: 0.4634\n",
            "Epoch 7, Batch 200, Loss: 0.4408\n",
            "Epoch 7, Batch 300, Loss: 0.1469\n",
            "Epoch 7, Batch 400, Loss: 0.1697\n",
            "Epoch 7, Batch 500, Loss: 0.3767\n",
            "Epoch 7, Batch 600, Loss: 0.2803\n",
            "Epoch 7, Batch 700, Loss: 0.2271\n",
            "Epoch 7, Batch 800, Loss: 0.1311\n",
            "Epoch 7, Batch 900, Loss: 0.2142\n",
            "Epoch 8, Batch 0, Loss: 0.3067\n",
            "Epoch 8, Batch 100, Loss: 0.2074\n",
            "Epoch 8, Batch 200, Loss: 0.2418\n",
            "Epoch 8, Batch 300, Loss: 0.1122\n",
            "Epoch 8, Batch 400, Loss: 0.1671\n",
            "Epoch 8, Batch 500, Loss: 0.2391\n",
            "Epoch 8, Batch 600, Loss: 0.3402\n",
            "Epoch 8, Batch 700, Loss: 0.3265\n",
            "Epoch 8, Batch 800, Loss: 0.3869\n",
            "Epoch 8, Batch 900, Loss: 0.1567\n",
            "Epoch 9, Batch 0, Loss: 0.2398\n",
            "Epoch 9, Batch 100, Loss: 0.2289\n",
            "Epoch 9, Batch 200, Loss: 0.2222\n",
            "Epoch 9, Batch 300, Loss: 0.1346\n",
            "Epoch 9, Batch 400, Loss: 0.2421\n",
            "Epoch 9, Batch 500, Loss: 0.1687\n",
            "Epoch 9, Batch 600, Loss: 0.2886\n",
            "Epoch 9, Batch 700, Loss: 0.2102\n",
            "Epoch 9, Batch 800, Loss: 0.2650\n",
            "Epoch 9, Batch 900, Loss: 0.2923\n",
            "Epoch 10, Batch 0, Loss: 0.2128\n",
            "Epoch 10, Batch 100, Loss: 0.2132\n",
            "Epoch 10, Batch 200, Loss: 0.2406\n",
            "Epoch 10, Batch 300, Loss: 0.2272\n",
            "Epoch 10, Batch 400, Loss: 0.2258\n",
            "Epoch 10, Batch 500, Loss: 0.1469\n",
            "Epoch 10, Batch 600, Loss: 0.1435\n",
            "Epoch 10, Batch 700, Loss: 0.4617\n",
            "Epoch 10, Batch 800, Loss: 0.1347\n",
            "Epoch 10, Batch 900, Loss: 0.3854\n",
            "Epoch 11, Batch 0, Loss: 0.2620\n",
            "Epoch 11, Batch 100, Loss: 0.3890\n",
            "Epoch 11, Batch 200, Loss: 0.0904\n",
            "Epoch 11, Batch 300, Loss: 0.1837\n",
            "Epoch 11, Batch 400, Loss: 0.2314\n",
            "Epoch 11, Batch 500, Loss: 0.3919\n",
            "Epoch 11, Batch 600, Loss: 0.2667\n",
            "Epoch 11, Batch 700, Loss: 0.2115\n",
            "Epoch 11, Batch 800, Loss: 0.2653\n",
            "Epoch 11, Batch 900, Loss: 0.1738\n",
            "Epoch 12, Batch 0, Loss: 0.1078\n",
            "Epoch 12, Batch 100, Loss: 0.1686\n",
            "Epoch 12, Batch 200, Loss: 0.1919\n",
            "Epoch 12, Batch 300, Loss: 0.2052\n",
            "Epoch 12, Batch 400, Loss: 0.2062\n",
            "Epoch 12, Batch 500, Loss: 0.2965\n",
            "Epoch 12, Batch 600, Loss: 0.2550\n",
            "Epoch 12, Batch 700, Loss: 0.2746\n",
            "Epoch 12, Batch 800, Loss: 0.3200\n",
            "Epoch 12, Batch 900, Loss: 0.2369\n",
            "Epoch 13, Batch 0, Loss: 0.2174\n",
            "Epoch 13, Batch 100, Loss: 0.2224\n",
            "Epoch 13, Batch 200, Loss: 0.1741\n",
            "Epoch 13, Batch 300, Loss: 0.2469\n",
            "Epoch 13, Batch 400, Loss: 0.1554\n",
            "Epoch 13, Batch 500, Loss: 0.2723\n",
            "Epoch 13, Batch 600, Loss: 0.2177\n",
            "Epoch 13, Batch 700, Loss: 0.2284\n",
            "Epoch 13, Batch 800, Loss: 0.1395\n",
            "Epoch 13, Batch 900, Loss: 0.1325\n",
            "Epoch 14, Batch 0, Loss: 0.1496\n",
            "Epoch 14, Batch 100, Loss: 0.2613\n",
            "Epoch 14, Batch 200, Loss: 0.2212\n",
            "Epoch 14, Batch 300, Loss: 0.1991\n",
            "Epoch 14, Batch 400, Loss: 0.4332\n",
            "Epoch 14, Batch 500, Loss: 0.2392\n",
            "Epoch 14, Batch 600, Loss: 0.3170\n",
            "Epoch 14, Batch 700, Loss: 0.1129\n",
            "Epoch 14, Batch 800, Loss: 0.1355\n",
            "Epoch 14, Batch 900, Loss: 0.2915\n",
            "Epoch 15, Batch 0, Loss: 0.3277\n",
            "Epoch 15, Batch 100, Loss: 0.1910\n",
            "Epoch 15, Batch 200, Loss: 0.1973\n",
            "Epoch 15, Batch 300, Loss: 0.2416\n",
            "Epoch 15, Batch 400, Loss: 0.1225\n",
            "Epoch 15, Batch 500, Loss: 0.2856\n",
            "Epoch 15, Batch 600, Loss: 0.2079\n",
            "Epoch 15, Batch 700, Loss: 0.2969\n",
            "Epoch 15, Batch 800, Loss: 0.2494\n",
            "Epoch 15, Batch 900, Loss: 0.1795\n",
            "Epoch 16, Batch 0, Loss: 0.2205\n",
            "Epoch 16, Batch 100, Loss: 0.2522\n",
            "Epoch 16, Batch 200, Loss: 0.2357\n",
            "Epoch 16, Batch 300, Loss: 0.3064\n",
            "Epoch 16, Batch 400, Loss: 0.5007\n",
            "Epoch 16, Batch 500, Loss: 0.4277\n",
            "Epoch 16, Batch 600, Loss: 0.1458\n",
            "Epoch 16, Batch 700, Loss: 0.2707\n",
            "Epoch 16, Batch 800, Loss: 0.3483\n",
            "Epoch 16, Batch 900, Loss: 0.4484\n",
            "Epoch 17, Batch 0, Loss: 0.1687\n",
            "Epoch 17, Batch 100, Loss: 0.2358\n",
            "Epoch 17, Batch 200, Loss: 0.2271\n",
            "Epoch 17, Batch 300, Loss: 0.2009\n",
            "Epoch 17, Batch 400, Loss: 0.4470\n",
            "Epoch 17, Batch 500, Loss: 0.1532\n",
            "Epoch 17, Batch 600, Loss: 0.1719\n",
            "Epoch 17, Batch 700, Loss: 0.1860\n",
            "Epoch 17, Batch 800, Loss: 0.1816\n",
            "Epoch 17, Batch 900, Loss: 0.1878\n",
            "Epoch 18, Batch 0, Loss: 0.1892\n",
            "Epoch 18, Batch 100, Loss: 0.1906\n",
            "Epoch 18, Batch 200, Loss: 0.1007\n",
            "Epoch 18, Batch 300, Loss: 0.2113\n",
            "Epoch 18, Batch 400, Loss: 0.2228\n",
            "Epoch 18, Batch 500, Loss: 0.2505\n",
            "Epoch 18, Batch 600, Loss: 0.2149\n",
            "Epoch 18, Batch 700, Loss: 0.1850\n",
            "Epoch 18, Batch 800, Loss: 0.1560\n",
            "Epoch 18, Batch 900, Loss: 0.2470\n",
            "Epoch 19, Batch 0, Loss: 0.1044\n",
            "Epoch 19, Batch 100, Loss: 0.3709\n",
            "Epoch 19, Batch 200, Loss: 0.1608\n",
            "Epoch 19, Batch 300, Loss: 0.1757\n",
            "Epoch 19, Batch 400, Loss: 0.1378\n",
            "Epoch 19, Batch 500, Loss: 0.2475\n",
            "Epoch 19, Batch 600, Loss: 0.3137\n",
            "Epoch 19, Batch 700, Loss: 0.3477\n",
            "Epoch 19, Batch 800, Loss: 0.3567\n",
            "Epoch 19, Batch 900, Loss: 0.2517\n",
            "Epoch 20, Batch 0, Loss: 0.0974\n",
            "Epoch 20, Batch 100, Loss: 0.1151\n",
            "Epoch 20, Batch 200, Loss: 0.2503\n",
            "Epoch 20, Batch 300, Loss: 0.2893\n",
            "Epoch 20, Batch 400, Loss: 0.1219\n",
            "Epoch 20, Batch 500, Loss: 0.1120\n",
            "Epoch 20, Batch 600, Loss: 0.1412\n",
            "Epoch 20, Batch 700, Loss: 0.2733\n",
            "Epoch 20, Batch 800, Loss: 0.1584\n",
            "Epoch 20, Batch 900, Loss: 0.1106\n",
            "Epoch 21, Batch 0, Loss: 0.2147\n",
            "Epoch 21, Batch 100, Loss: 0.1760\n",
            "Epoch 21, Batch 200, Loss: 0.3380\n",
            "Epoch 21, Batch 300, Loss: 0.1987\n",
            "Epoch 21, Batch 400, Loss: 0.2272\n",
            "Epoch 21, Batch 500, Loss: 0.3318\n",
            "Epoch 21, Batch 600, Loss: 0.3101\n",
            "Epoch 21, Batch 700, Loss: 0.2199\n",
            "Epoch 21, Batch 800, Loss: 0.1731\n",
            "Epoch 21, Batch 900, Loss: 0.1985\n",
            "Epoch 22, Batch 0, Loss: 0.1577\n",
            "Epoch 22, Batch 100, Loss: 0.1555\n",
            "Epoch 22, Batch 200, Loss: 0.1527\n",
            "Epoch 22, Batch 300, Loss: 0.2458\n",
            "Epoch 22, Batch 400, Loss: 0.2775\n",
            "Epoch 22, Batch 500, Loss: 0.2871\n",
            "Epoch 22, Batch 600, Loss: 0.1531\n",
            "Epoch 22, Batch 700, Loss: 0.2634\n",
            "Epoch 22, Batch 800, Loss: 0.1640\n",
            "Epoch 22, Batch 900, Loss: 0.2151\n",
            "Epoch 23, Batch 0, Loss: 0.1315\n",
            "Epoch 23, Batch 100, Loss: 0.2307\n",
            "Epoch 23, Batch 200, Loss: 0.2014\n",
            "Epoch 23, Batch 300, Loss: 0.1532\n",
            "Epoch 23, Batch 400, Loss: 0.1595\n",
            "Epoch 23, Batch 500, Loss: 0.1578\n",
            "Epoch 23, Batch 600, Loss: 0.3639\n",
            "Epoch 23, Batch 700, Loss: 0.3324\n",
            "Epoch 23, Batch 800, Loss: 0.1415\n",
            "Epoch 23, Batch 900, Loss: 0.1504\n",
            "Epoch 24, Batch 0, Loss: 0.2581\n",
            "Epoch 24, Batch 100, Loss: 0.2231\n",
            "Epoch 24, Batch 200, Loss: 0.1209\n",
            "Epoch 24, Batch 300, Loss: 0.2210\n",
            "Epoch 24, Batch 400, Loss: 0.1791\n",
            "Epoch 24, Batch 500, Loss: 0.2359\n",
            "Epoch 24, Batch 600, Loss: 0.1489\n",
            "Epoch 24, Batch 700, Loss: 0.2861\n",
            "Epoch 24, Batch 800, Loss: 0.2066\n",
            "Epoch 24, Batch 900, Loss: 0.1600\n",
            "Epoch 25, Batch 0, Loss: 0.1765\n",
            "Epoch 25, Batch 100, Loss: 0.1324\n",
            "Epoch 25, Batch 200, Loss: 0.2156\n",
            "Epoch 25, Batch 300, Loss: 0.1191\n",
            "Epoch 25, Batch 400, Loss: 0.2439\n",
            "Epoch 25, Batch 500, Loss: 0.1543\n",
            "Epoch 25, Batch 600, Loss: 0.1508\n",
            "Epoch 25, Batch 700, Loss: 0.1642\n",
            "Epoch 25, Batch 800, Loss: 0.2004\n",
            "Epoch 25, Batch 900, Loss: 0.1640\n",
            "Epoch 26, Batch 0, Loss: 0.2098\n",
            "Epoch 26, Batch 100, Loss: 0.1851\n",
            "Epoch 26, Batch 200, Loss: 0.1293\n",
            "Epoch 26, Batch 300, Loss: 0.3619\n",
            "Epoch 26, Batch 400, Loss: 0.1057\n",
            "Epoch 26, Batch 500, Loss: 0.0988\n",
            "Epoch 26, Batch 600, Loss: 0.1198\n",
            "Epoch 26, Batch 700, Loss: 0.1941\n",
            "Epoch 26, Batch 800, Loss: 0.1790\n",
            "Epoch 26, Batch 900, Loss: 0.1069\n",
            "Epoch 27, Batch 0, Loss: 0.2008\n",
            "Epoch 27, Batch 100, Loss: 0.1521\n",
            "Epoch 27, Batch 200, Loss: 0.1361\n",
            "Epoch 27, Batch 300, Loss: 0.1986\n",
            "Epoch 27, Batch 400, Loss: 0.1580\n",
            "Epoch 27, Batch 500, Loss: 0.2616\n",
            "Epoch 27, Batch 600, Loss: 0.2280\n",
            "Epoch 27, Batch 700, Loss: 0.3416\n",
            "Epoch 27, Batch 800, Loss: 0.1983\n",
            "Epoch 27, Batch 900, Loss: 0.1419\n",
            "Epoch 28, Batch 0, Loss: 0.2139\n",
            "Epoch 28, Batch 100, Loss: 0.2013\n",
            "Epoch 28, Batch 200, Loss: 0.0991\n",
            "Epoch 28, Batch 300, Loss: 0.1643\n",
            "Epoch 28, Batch 400, Loss: 0.1480\n",
            "Epoch 28, Batch 500, Loss: 0.0698\n",
            "Epoch 28, Batch 600, Loss: 0.1573\n",
            "Epoch 28, Batch 700, Loss: 0.1873\n",
            "Epoch 28, Batch 800, Loss: 0.2401\n",
            "Epoch 28, Batch 900, Loss: 0.1125\n",
            "Epoch 29, Batch 0, Loss: 0.2047\n",
            "Epoch 29, Batch 100, Loss: 0.1878\n",
            "Epoch 29, Batch 200, Loss: 0.1062\n",
            "Epoch 29, Batch 300, Loss: 0.1242\n",
            "Epoch 29, Batch 400, Loss: 0.2416\n",
            "Epoch 29, Batch 500, Loss: 0.2144\n",
            "Epoch 29, Batch 600, Loss: 0.1078\n",
            "Epoch 29, Batch 700, Loss: 0.0466\n",
            "Epoch 29, Batch 800, Loss: 0.1237\n",
            "Epoch 29, Batch 900, Loss: 0.2371\n",
            "Epoch 30, Batch 0, Loss: 0.1876\n",
            "Epoch 30, Batch 100, Loss: 0.3049\n",
            "Epoch 30, Batch 200, Loss: 0.1796\n",
            "Epoch 30, Batch 300, Loss: 0.1501\n",
            "Epoch 30, Batch 400, Loss: 0.2335\n",
            "Epoch 30, Batch 500, Loss: 0.2183\n",
            "Epoch 30, Batch 600, Loss: 0.2632\n",
            "Epoch 30, Batch 700, Loss: 0.1910\n",
            "Epoch 30, Batch 800, Loss: 0.1226\n",
            "Epoch 30, Batch 900, Loss: 0.1777\n",
            "Epoch 31, Batch 0, Loss: 0.0956\n",
            "Epoch 31, Batch 100, Loss: 0.1925\n",
            "Epoch 31, Batch 200, Loss: 0.1309\n",
            "Epoch 31, Batch 300, Loss: 0.0856\n",
            "Epoch 31, Batch 400, Loss: 0.1830\n",
            "Epoch 31, Batch 500, Loss: 0.1216\n",
            "Epoch 31, Batch 600, Loss: 0.1282\n",
            "Epoch 31, Batch 700, Loss: 0.1774\n",
            "Epoch 31, Batch 800, Loss: 0.3881\n",
            "Epoch 31, Batch 900, Loss: 0.1328\n",
            "Epoch 32, Batch 0, Loss: 0.0879\n",
            "Epoch 32, Batch 100, Loss: 0.1012\n",
            "Epoch 32, Batch 200, Loss: 0.2165\n",
            "Epoch 32, Batch 300, Loss: 0.1319\n",
            "Epoch 32, Batch 400, Loss: 0.1945\n",
            "Epoch 32, Batch 500, Loss: 0.1587\n",
            "Epoch 32, Batch 600, Loss: 0.3484\n",
            "Epoch 32, Batch 700, Loss: 0.1036\n",
            "Epoch 32, Batch 800, Loss: 0.3433\n",
            "Epoch 32, Batch 900, Loss: 0.1321\n",
            "Epoch 33, Batch 0, Loss: 0.1264\n",
            "Epoch 33, Batch 100, Loss: 0.1017\n",
            "Epoch 33, Batch 200, Loss: 0.1826\n",
            "Epoch 33, Batch 300, Loss: 0.2768\n",
            "Epoch 33, Batch 400, Loss: 0.1759\n",
            "Epoch 33, Batch 500, Loss: 0.2312\n",
            "Epoch 33, Batch 600, Loss: 0.0679\n",
            "Epoch 33, Batch 700, Loss: 0.1271\n",
            "Epoch 33, Batch 800, Loss: 0.0784\n",
            "Epoch 33, Batch 900, Loss: 0.2354\n",
            "Epoch 34, Batch 0, Loss: 0.2313\n",
            "Epoch 34, Batch 100, Loss: 0.1549\n",
            "Epoch 34, Batch 200, Loss: 0.0985\n",
            "Epoch 34, Batch 300, Loss: 0.2358\n",
            "Epoch 34, Batch 400, Loss: 0.3235\n",
            "Epoch 34, Batch 500, Loss: 0.1463\n",
            "Epoch 34, Batch 600, Loss: 0.1352\n",
            "Epoch 34, Batch 700, Loss: 0.1360\n",
            "Epoch 34, Batch 800, Loss: 0.1602\n",
            "Epoch 34, Batch 900, Loss: 0.2620\n",
            "Epoch 35, Batch 0, Loss: 0.0831\n",
            "Epoch 35, Batch 100, Loss: 0.1942\n",
            "Epoch 35, Batch 200, Loss: 0.1457\n",
            "Epoch 35, Batch 300, Loss: 0.1710\n",
            "Epoch 35, Batch 400, Loss: 0.1422\n",
            "Epoch 35, Batch 500, Loss: 0.1522\n",
            "Epoch 35, Batch 600, Loss: 0.1724\n",
            "Epoch 35, Batch 700, Loss: 0.1777\n",
            "Epoch 35, Batch 800, Loss: 0.2180\n",
            "Epoch 35, Batch 900, Loss: 0.1287\n",
            "Epoch 36, Batch 0, Loss: 0.2258\n",
            "Epoch 36, Batch 100, Loss: 0.1582\n",
            "Epoch 36, Batch 200, Loss: 0.0774\n",
            "Epoch 36, Batch 300, Loss: 0.3697\n",
            "Epoch 36, Batch 400, Loss: 0.1512\n",
            "Epoch 36, Batch 500, Loss: 0.0912\n",
            "Epoch 36, Batch 600, Loss: 0.0914\n",
            "Epoch 36, Batch 700, Loss: 0.1121\n",
            "Epoch 36, Batch 800, Loss: 0.1942\n",
            "Epoch 36, Batch 900, Loss: 0.1390\n",
            "Epoch 37, Batch 0, Loss: 0.1686\n",
            "Epoch 37, Batch 100, Loss: 0.1771\n",
            "Epoch 37, Batch 200, Loss: 0.2576\n",
            "Epoch 37, Batch 300, Loss: 0.1735\n",
            "Epoch 37, Batch 400, Loss: 0.1336\n",
            "Epoch 37, Batch 500, Loss: 0.3136\n",
            "Epoch 37, Batch 600, Loss: 0.1399\n",
            "Epoch 37, Batch 700, Loss: 0.3822\n",
            "Epoch 37, Batch 800, Loss: 0.2656\n",
            "Epoch 37, Batch 900, Loss: 0.1746\n",
            "Epoch 38, Batch 0, Loss: 0.0983\n",
            "Epoch 38, Batch 100, Loss: 0.2402\n",
            "Epoch 38, Batch 200, Loss: 0.0924\n",
            "Epoch 38, Batch 300, Loss: 0.1029\n",
            "Epoch 38, Batch 400, Loss: 0.1667\n",
            "Epoch 38, Batch 500, Loss: 0.1324\n",
            "Epoch 38, Batch 600, Loss: 0.0922\n",
            "Epoch 38, Batch 700, Loss: 0.2795\n",
            "Epoch 38, Batch 800, Loss: 0.1518\n",
            "Epoch 38, Batch 900, Loss: 0.1822\n",
            "Epoch 39, Batch 0, Loss: 0.2123\n",
            "Epoch 39, Batch 100, Loss: 0.1447\n",
            "Epoch 39, Batch 200, Loss: 0.1927\n",
            "Epoch 39, Batch 300, Loss: 0.1095\n",
            "Epoch 39, Batch 400, Loss: 0.1176\n",
            "Epoch 39, Batch 500, Loss: 0.1940\n",
            "Epoch 39, Batch 600, Loss: 0.1931\n",
            "Epoch 39, Batch 700, Loss: 0.1281\n",
            "Epoch 39, Batch 800, Loss: 0.2017\n",
            "Epoch 39, Batch 900, Loss: 0.1151\n",
            "Epoch 40, Batch 0, Loss: 0.2099\n",
            "Epoch 40, Batch 100, Loss: 0.1673\n",
            "Epoch 40, Batch 200, Loss: 0.2131\n",
            "Epoch 40, Batch 300, Loss: 0.1702\n",
            "Epoch 40, Batch 400, Loss: 0.2000\n",
            "Epoch 40, Batch 500, Loss: 0.2024\n",
            "Epoch 40, Batch 600, Loss: 0.3039\n",
            "Epoch 40, Batch 700, Loss: 0.1805\n",
            "Epoch 40, Batch 800, Loss: 0.1149\n",
            "Epoch 40, Batch 900, Loss: 0.1336\n",
            "Epoch 41, Batch 0, Loss: 0.1089\n",
            "Epoch 41, Batch 100, Loss: 0.1223\n",
            "Epoch 41, Batch 200, Loss: 0.1802\n",
            "Epoch 41, Batch 300, Loss: 0.1293\n",
            "Epoch 41, Batch 400, Loss: 0.2647\n",
            "Epoch 41, Batch 500, Loss: 0.2362\n",
            "Epoch 41, Batch 600, Loss: 0.2070\n",
            "Epoch 41, Batch 700, Loss: 0.1520\n",
            "Epoch 41, Batch 800, Loss: 0.1484\n",
            "Epoch 41, Batch 900, Loss: 0.1625\n",
            "Epoch 42, Batch 0, Loss: 0.1528\n",
            "Epoch 42, Batch 100, Loss: 0.1706\n",
            "Epoch 42, Batch 200, Loss: 0.1283\n",
            "Epoch 42, Batch 300, Loss: 0.0571\n",
            "Epoch 42, Batch 400, Loss: 0.1451\n",
            "Epoch 42, Batch 500, Loss: 0.1738\n",
            "Epoch 42, Batch 600, Loss: 0.2906\n",
            "Epoch 42, Batch 700, Loss: 0.1436\n",
            "Epoch 42, Batch 800, Loss: 0.1245\n",
            "Epoch 42, Batch 900, Loss: 0.1357\n",
            "Epoch 43, Batch 0, Loss: 0.1498\n",
            "Epoch 43, Batch 100, Loss: 0.1943\n",
            "Epoch 43, Batch 200, Loss: 0.2279\n",
            "Epoch 43, Batch 300, Loss: 0.1404\n",
            "Epoch 43, Batch 400, Loss: 0.2005\n",
            "Epoch 43, Batch 500, Loss: 0.1190\n",
            "Epoch 43, Batch 600, Loss: 0.0950\n",
            "Epoch 43, Batch 700, Loss: 0.2716\n",
            "Epoch 43, Batch 800, Loss: 0.3044\n",
            "Epoch 43, Batch 900, Loss: 0.2086\n",
            "Epoch 44, Batch 0, Loss: 0.0857\n",
            "Epoch 44, Batch 100, Loss: 0.2459\n",
            "Epoch 44, Batch 200, Loss: 0.0997\n",
            "Epoch 44, Batch 300, Loss: 0.1813\n",
            "Epoch 44, Batch 400, Loss: 0.2639\n",
            "Epoch 44, Batch 500, Loss: 0.2071\n",
            "Epoch 44, Batch 600, Loss: 0.2572\n",
            "Epoch 44, Batch 700, Loss: 0.1891\n",
            "Epoch 44, Batch 800, Loss: 0.0722\n",
            "Epoch 44, Batch 900, Loss: 0.1437\n",
            "Epoch 45, Batch 0, Loss: 0.1515\n",
            "Epoch 45, Batch 100, Loss: 0.1354\n",
            "Epoch 45, Batch 200, Loss: 0.2769\n",
            "Epoch 45, Batch 300, Loss: 0.1391\n",
            "Epoch 45, Batch 400, Loss: 0.0861\n",
            "Epoch 45, Batch 500, Loss: 0.4145\n",
            "Epoch 45, Batch 600, Loss: 0.1843\n",
            "Epoch 45, Batch 700, Loss: 0.1503\n",
            "Epoch 45, Batch 800, Loss: 0.1631\n",
            "Epoch 45, Batch 900, Loss: 0.1493\n",
            "Epoch 46, Batch 0, Loss: 0.0942\n",
            "Epoch 46, Batch 100, Loss: 0.1366\n",
            "Epoch 46, Batch 200, Loss: 0.1578\n",
            "Epoch 46, Batch 300, Loss: 0.1246\n",
            "Epoch 46, Batch 400, Loss: 0.1099\n",
            "Epoch 46, Batch 500, Loss: 0.1540\n",
            "Epoch 46, Batch 600, Loss: 0.1308\n",
            "Epoch 46, Batch 700, Loss: 0.1833\n",
            "Epoch 46, Batch 800, Loss: 0.1748\n",
            "Epoch 46, Batch 900, Loss: 0.1879\n",
            "Epoch 47, Batch 0, Loss: 0.1098\n",
            "Epoch 47, Batch 100, Loss: 0.0452\n",
            "Epoch 47, Batch 200, Loss: 0.2283\n",
            "Epoch 47, Batch 300, Loss: 0.0751\n",
            "Epoch 47, Batch 400, Loss: 0.2791\n",
            "Epoch 47, Batch 500, Loss: 0.1962\n",
            "Epoch 47, Batch 600, Loss: 0.4154\n",
            "Epoch 47, Batch 700, Loss: 0.2245\n",
            "Epoch 47, Batch 800, Loss: 0.1903\n",
            "Epoch 47, Batch 900, Loss: 0.2736\n",
            "Epoch 48, Batch 0, Loss: 0.1626\n",
            "Epoch 48, Batch 100, Loss: 0.1537\n",
            "Epoch 48, Batch 200, Loss: 0.1033\n",
            "Epoch 48, Batch 300, Loss: 0.1255\n",
            "Epoch 48, Batch 400, Loss: 0.2755\n",
            "Epoch 48, Batch 500, Loss: 0.1037\n",
            "Epoch 48, Batch 600, Loss: 0.0928\n",
            "Epoch 48, Batch 700, Loss: 0.0842\n",
            "Epoch 48, Batch 800, Loss: 0.1207\n",
            "Epoch 48, Batch 900, Loss: 0.0388\n",
            "Epoch 49, Batch 0, Loss: 0.1049\n",
            "Epoch 49, Batch 100, Loss: 0.1153\n",
            "Epoch 49, Batch 200, Loss: 0.1216\n",
            "Epoch 49, Batch 300, Loss: 0.1646\n",
            "Epoch 49, Batch 400, Loss: 0.1009\n",
            "Epoch 49, Batch 500, Loss: 0.1390\n",
            "Epoch 49, Batch 600, Loss: 0.1174\n",
            "Epoch 49, Batch 700, Loss: 0.1745\n",
            "Epoch 49, Batch 800, Loss: 0.2279\n",
            "Epoch 49, Batch 900, Loss: 0.1017\n",
            "Epoch 50, Batch 0, Loss: 0.0908\n",
            "Epoch 50, Batch 100, Loss: 0.0705\n",
            "Epoch 50, Batch 200, Loss: 0.0886\n",
            "Epoch 50, Batch 300, Loss: 0.1053\n",
            "Epoch 50, Batch 400, Loss: 0.0697\n",
            "Epoch 50, Batch 500, Loss: 0.0910\n",
            "Epoch 50, Batch 600, Loss: 0.1179\n",
            "Epoch 50, Batch 700, Loss: 0.1573\n",
            "Epoch 50, Batch 800, Loss: 0.1166\n",
            "Epoch 50, Batch 900, Loss: 0.1272\n",
            "Training complete!\n",
            "Test Accuracy: 87.72%\n"
          ]
        }
      ],
      "source": [
        "# Train and test\n",
        "train(model, train_loader, loss_fn, optimizer, epochs=50)\n",
        "test(model, test_loader, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8yKukk4zxx9O"
      },
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "torch.save(model.state_dict(), \"Assignment3.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdzBY6MqyA4N",
        "outputId": "9475506d-443f-4749-84fb-bd5556a2e1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "Test Accuracy: 87.72%\n"
          ]
        }
      ],
      "source": [
        "# Load and test the model using saved weights\n",
        "def load_and_test():\n",
        "    model.load_state_dict(torch.load(\"Assignment3.pth\"))\n",
        "    model.eval()\n",
        "    test(model, test_loader, loss_fn)\n",
        "\n",
        "load_and_test()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}